@article{Quenouille1949,
author = {Quenouille, M. H.},
doi = {10.1111/j.2517-6161.1949.tb00023.x},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
month = {jan},
number = {1},
pages = {68--84},
title = {{Approximate Tests of Correlation in Time-Series}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1949.tb00023.x},
volume = {11},
year = {1949}
}
@article{Hall1987,
abstract = {4, 1491-1519 ON -LEIBLER LOSS AND DENSITY ESTIMATION By Peter Hall Australian National University "Discrimination information," or -Leibler loss},
author = {Hall, Peter},
doi = {10.1214/aos/1176350606},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {dec},
number = {4},
pages = {1491--1519},
title = {{On Kullback-Leibler Loss and Density Estimation}},
url = {http://projecteuclid.org/euclid.aos/1176350606},
volume = {15},
year = {1987}
}
@book{Hardle2004,
address = {Berlin, Heidelberg},
author = {H{\"{a}}rdle, Wolfgang and Werwatz, Axel and M{\"{u}}ller, Marlene and Sperlich, Stefan},
doi = {10.1007/978-3-642-17146-8},
isbn = {978-3-642-62076-8},
pages = {xxviii+299},
publisher = {Springer Berlin Heidelberg},
series = {Springer Series in Statistics},
title = {{Nonparametric and Semiparametric Models}},
url = {http://link.springer.com/10.1007/978-3-642-17146-8},
year = {2004}
}
@article{Efron1979,
abstract = {We discuss the following problem: given a random sample X=(X1,X2,⋯,Xn)X=(X1,X2,⋯,Xn)\mathbf{X} = (X_1, X_2, \cdots, X_n) from an unknown probability distribution FFF, estimate the sampling distribution of some prespecified random variable R(X,F)R(X,F)R(\mathbf{X}, F), on the basis of the observed data xx\mathbf{x}. (Standard jackknife theory gives an approximate mean and variance in the case R(X,F)=$\theta$(F^)−$\theta$(F),$\theta$R(X,F)=$\theta$(F^)−$\theta$(F),$\theta$R(\mathbf{X}, F) = \theta(\hat{F}) - \theta(F), \theta some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
author = {Efron, B.},
doi = {10.1214/aos/1176344552},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {jan},
number = {1},
pages = {1--26},
title = {{Bootstrap Methods: Another Look at the Jackknife}},
url = {http://projecteuclid.org/euclid.aos/1176344552},
volume = {7},
year = {1979}
}
