 <<echo=FALSE>>=
rm(list=ls())
@

<<set-parent, echo=FALSE, cache=FALSE>>=
knitr::set_parent('main-CA-403_notas.Rtex')
@

\chapter{Estimación de densidades}

\section{Histograma}

El histograma es una de las estructuras básicas en estadística. Básicamente con este objeto se puede visualizar la distribución de los datos sin tener conocimiento previo de los mismos.

\subsection{Construcción Estadística}

Suponga que  \(X_1,X_2, \dots ,X_n\) proviene de una distribución desconocida.

\begin{itemize}
	\item Seleccione un origen \(x_0\) y divida la linea real en \emph{segmentos}.
	      \begin{equation*}
		      B_j = [x_0 +(j - 1)h,x_0 + jh) \quad j\in \mathbb{Z}
	      \end{equation*}

	\item Cuente cuántas observaciones caen en cada segmento. \(n_j\).

	      <<observaciones-histograma, echo=FALSE,fig.asp=0.2>>=

	      df <- data.frame(x = rnorm(500), y = rep(0.005, 500))

	      ggplot(df) +
	      geom_histogram(mapping = aes(x = x),
	      fill = "white",
	      color = "red3") +
	      geom_jitter(aes(x, y),
	      width = 0,
	      height = 0.0045,
	      color = "blue4") +
	      coord_cartesian(ylim = c(0, 0.01)) +
	      theme_minimal() +
	      theme(
	      axis.line.y = element_blank(),
	      axis.ticks.y = element_blank(),
	      axis.text.y = element_blank(),
	      axis.title.y = element_blank()
	      )

	      @
	\item Cuente la frecuencia por el tamaño de muestra \(n\) y el ancho de banda \(h\).
	      \begin{equation*}
		      f_j = \frac{n_j}{nh}
	      \end{equation*}

	\item  Dibuje el histograma.

	      <<ejemplo-inicial-histograma, echo=FALSE>>=

	      df <- data.frame(x = rnorm(500), y = rep(20, 500))

	      ggplot(df) +
	      geom_histogram(mapping = aes(x = x),
	      fill = "white",
	      color = "red3") +
	      geom_rug(aes(x), col = "blue4") +
	      theme_minimal()
	      @

\end{itemize}

Formalmente el histograma es el

\begin{equation*}
	\hat{f}_h(x) = \frac{1}{nh} \sum_{i = 1}^{n} \sum_{j} I(X_i\in B_j) I(x\in B_j),
\end{equation*}

donde \(I\) es la indicadora.

\subsection{Construcción probabilistica}

Denote \(m_j=jh-h/2\) el centro del segmento,

\begin{align*}
	\mathbb{P}\left(X\in \left[m_j - \frac{h}{2},m_j + \frac{h}{2} \right)\right) & =
	\int_{m_j - \frac{h}{2}}^{m_j + \frac{h}{2}} f(u)du                                             \\
	                                                                              & \approx f(m_j)h
\end{align*}

Esto se puede aproximar como

\begin{equation*}
	\mathbb{P} \left(X\in \left[m_j - \frac{h}{2},m_j + \frac{h}{2}\right) \right)  \approx   \frac{1}{n} \#
	\left\{X\in \left[m_j - \frac{h}{2},m_j + \frac{h}{2}\right) \right\}
\end{equation*}

Acomodando un poco la expresión

\begin{equation*}
	\hat{f}_h(m_j) =  \frac{1}{nh} \#
	\left\{X\in \left[m_j - \frac{h}{2},m_j + \frac{h}{2}\right) \right\}
\end{equation*}

\subsection{Propiedades estadísticas }

Suponga que  \(x_0 = 0\) y que \(x \in B_j\) fijo, entonces

\begin{equation*}
	\hat{f}_h(m_j) =  \frac{1}{nh} \sum_{i = 1}^{n} I(X_i \in B_j)
\end{equation*}

\subsubsection{Sesgo}

El cálculo del sesgo es el

\begin{align*}
	\mathbb{E}\left[ \hat{f}_h(m_j)\right]
	  & =  \frac{1}{nh} \sum_{i = 1}^{n} \mathbb{E}\left[ I(X_i \in B_j)\right] \\
	  & = \frac{1}{nh} n \mathbb{E}\left[ I(X_i \in B_j)\right]
\end{align*}

\(I(X_i \in B_j)\) es una indicadora con probabilidad de 1 de \(\int_{(j -
	1)h}^{jh} f(u)du\) y 0 sino.

Entonces

\begin{align*}
	\mathbb{E}\left[ I(X_i \in B_j)\right] = \mathbb{P}\left(I(X_i \in
	B_j)=1\right) = \int_{(j - 1)h}^{jh} f(u)du.
\end{align*}

Entonces,
\begin{align*}
	\mathbb{E}\left[{f}_h(m_j)\right]
	  & = \frac{1}{h} \int_{(j - 1)h}^{jh} f(u)du
\end{align*}

\begin{equation*}
	Sesgo(\hat{f}_h(m_j)) = \frac{1}{h} \int_{(j -
		1)h}^{jh} f(u)du - f(x)
\end{equation*}

Esto se puede aproximar usando Taylor alrededor del centro \(m_j = jh - h/2\) de \(B_j\) de modo que \(f(u) - f(x) \approx f^{\prime}(m_j)(u - x)\).

\begin{equation*}
	Sesgo(\hat{f}_h(m_j)) =  \frac{1}{h} \int_{(j -
		1)h}^{jh} f(u) - f(x) du \approx f^\prime(m_j)(m_j - x)
\end{equation*}

\subsubsection{Varianza}

Dado que todos los \(X_i\) son i.i.d., entonces

\begin{align*}
	\mathrm{Var}\left( \hat{f}_h(m_j)\right) & =
	\mathrm{Var}\left( \frac{1}{nh} \sum_{i = 1}^{n} I(X_i \in B_j)\right)                                  \\
	                                         & = \frac{1}{n^2h^2} n\mathrm{Var}\left( I(X_i \in B_j)\right)
\end{align*}

La variable \(I\) es una bernoulli con parametro \(\int_{(j - 1)h}^{h} f(u)du\) por lo tanto su varianza es el

\begin{equation*}
	\mathrm{Var}\left( \hat{f}_h(x)\right)\, =
	\frac{1}{nh^2} \left(\int_{(j - 1)h}^{h} f(u)du \right)\left( 1 -\int_{(j - 1)h}^{h} f(u)du \right)
\end{equation*}

\begin{tarea}{}{tarea_1}
	Usando un desarrollo de Taylor como en la parte anterior, pruebe que:
	\begin{equation*}
		\mathrm{Var}\left( \hat{f}_h(x)\right)\approx
		\frac{1}{nh} f(x)
	\end{equation*}
\end{tarea}

\subsection{Error cuadrático medio}

El error cuadrático medio del histograma es el

\begin{equation*}
	\mathrm{MSE}\left( \hat{f}_h(x)\right) =
	\mathrm{E}\left[\left(\hat{f}_h(x) - f(x)\right)^2\right] = \mathrm{Sesgo}^2\left( \hat{f}_h(x)\right) + \mathrm{Var}\left( \hat{f}_h(x)\right).
\end{equation*}

\begin{tarea}{}{tarea_2}
	¿Pueden probar la segunda igualdad de la expresión anterior?
\end{tarea}

\begin{solucion}{}{Pba_tarea2}
	Prueba segunda igualdad:
	\begin{align*}
		& \text{Sesgo}^2\left(\hat{f}_h(x)  \right) + \text{Var}\left( \hat{f}_h(x)\right)  = \\ & \left[ E\left(\hat{f}_h(x)\right) - f(x)\right]^2 + E\left[\left( E\left(\hat{f}_h(x)\right) - \hat{f}_h(x)\right)^2\right] \ =
		\\ & E\left[\left[ E\left(\hat{f}_h(x)\right) - f(x)\right]^2 + \left( E\left(\hat{f}_h(x)\right) - \hat{f}_h(x)\right)^2   \right] \ \textcolor{red}{(*)} \
	\end{align*}
	Ahora note que:
	\begin{align*}
		  & E\left[\left( E\left(\hat{f}_h(x)\right) - f(x)   \right) \left(E\left(\hat{f}_h(x)\right) - \hat{f}_h(x)    \right)    \right] \ = \                                      \\
		  & E\left[E\left(\hat{f}_h(x)\right)^2 \right] \ - \ E\left[E\left(\hat{f}_h(x)\right)\cdot \hat{f}_h(x) \right] \ - \ E\left[f(x)\cdot E\left(\hat{f}_h(x)\right)\right] \ + \\
		  & E\left[f(x)\cdot \hat{f}_h(x)\right]\ = \                                                                                                                                  \\
		  & E\left(\hat{f}_h(x)\right)^2  \ - \ E\left(\hat{f}_h(x)\right)^2  \ - \ E\left(\hat{f}_h(x)\right)\cdot E\left( f(x)\right) \ + \
		E\left( f(x)\right)\cdot E\left(\hat{f}_h(x)\right) \                                                                                                                         \\
		  & = 0
	\end{align*}
	Entonces:
	\begin{align*}
		  & \textcolor{red}{(*)} \ = \ E\left[\left[ E\left(\hat{f}_h(x)\right) - f(x)\right]^2 \ -  \right.                                                                                                         \\
		  & \left. \ 2\left( E\left(\hat{f}_h(x)\right) - f(x)   \right) \left(E\left(\hat{f}_h(x)\right) - \hat{f}_h(x)    \right) \ + \ \left( E\left(\hat{f}_h(x)\right) - \hat{f}_h(x)\right)^2   \right] \ = \  \\
		  & E\left[ \left(E\left(\hat{f}_h(x)\right) - f(x) \ - \ E\left(\hat{f}_h(x)\right) + \hat{f}_h(x) \right)^2   \right] \ = \                                                                                \\
		  & E\left[\left(\hat{f}_h(x) - f(x)\right)^2    \right]
	\end{align*}
	\qed
\end{solucion}

Retomando los términos anteriores se tiene que

\begin{multline*}
	\mathrm{MSE}\left( \hat{f}_h(x)\right) =
	\frac{1}{nh} f(x) + f^\prime
	\left\{
	\left(
	j - \frac{1}{2}
	\right) h
	\right\}^2
	\left\{
	\left(
	j - \frac{1}{2}
	\right) h - x
	\right\}^2 \\
	+ o\left(h \right) + 		o\left(\frac{1}{nh} \right)
\end{multline*}

\begin{nota}{}{}
	Si \(h \to 0\) y \(nh \to \infty\) entonces \(\mathrm{MSE}\left(  \hat{f}_h(x)\right) \to 0 \). Es decir, conforme usamos más observaciones, pero el ancho de banda de banda no decrece tan rápida, entonces el error cuadrático medio converge a 0.

	Esto indica que si \(\mathrm{MSE}\left(  \hat{f}_h(x)\right) \to 0 \) (convergencia en \(\mathbb{L}^2\)) implica que \(\hat{f}_h(x) \stackrel{\mathcal{P}}{\to} f(x)\), por lo tanto \(\hat{f}_h\) es consistente.
\end{nota}

La fórmula anterior tiene la siguiente particularidad

\begin{itemize}
	\item Si \(h\to 0\), la varianza crece (converge a \(\infty\)) y el sesgo decrece (converge a \(f^\prime (0)x^2\)).
	\item Si \(h\to \infty\), la varianza decrece (hacia 0)  y el sesgo crece (hacia \(\infty\))
\end{itemize}

Note que la figura \ref{fig:MSE-histograma}

<<echo=FALSE>>=
#Generate 1000 numbers with normal standard distribution
x <- rnorm(1000)
n <- length(x)

# Point to evaluate the MSE
x0 <- 0
# Real value of f(x0)
f_x0 <- dnorm(x0)

# || f’ ||_2^2 for a normal standard distribution
norm_f_prime <- 1 / (4 * sqrt(pi))

#Sequence of bins
hvec <- seq(0.1, 0.7, by = 0.0005)
Sesgo <- numeric()

Var <- numeric()

MSE <- numeric()

Sesgo_MISE <- numeric()

Var_MISE <- numeric()

MISE <- numeric()

for (h in hvec) {
		breaks <- floor((min(x)) / h):ceiling((max(x)) / h)
		breaks <- breaks * h
		xhist <- hist(x, breaks = breaks, plot = FALSE)
		# Average of bins near x0
		bins_near_x0 <- xhist$breaks <= x0 + h & xhist$breaks >= x0 - h

		p <- mean(xhist$counts[bins_near_x0]) / n

		# Expectation of \hat{f} in x0
		E_fhat_x0 <- p / h

		#Compute of Sesgo, Var, MSE and MISE
		Sesgo <- c(Sesgo, E_fhat_x0 - f_x0)

		Var <- c(Var, (p * (1 - p)) / (n * h ^ 2))
		MSE <- c(MSE, tail(Var, 1) + tail(Sesgo, 1) ^ 2)
		Var_MISE <- c(Var_MISE, 1 / (n * h))
		Sesgo_MISE <- c(Sesgo_MISE, h ^ 2 * norm_f_prime / 12)
		MISE <- c(MISE, tail(Var_MISE, 1) + tail(Sesgo_MISE, 1))
	}

df_MSE <- rbind(
data.frame(Medida = "Sesgo^2", h = hvec, Valor = Sesgo^2),
data.frame(Medida = "Var", h = hvec, Valor = Var),
data.frame(Medida = "MSE", h = hvec, Valor = MSE)
)

df_MSE$Medida  <-  as.factor(df_MSE$Medida)

df_MISE <- rbind(
data.frame(Medida = "Sesgo_MISE^2", h = hvec, Valor = Sesgo_MISE^2),
data.frame(Medida = "Var_MISE", h = hvec, Valor = Var_MISE),
data.frame(Medida = "MISE", h = hvec, Valor = MISE)
)

df_MISE$Medida  <-  as.factor(df_MISE$Medida)
@

<<MSE-histograma, echo=FALSE>>=
ggplot(df_MSE) +
geom_line(aes(
x = h,
y = Valor,
color = Medida,
linetype = Medida
), size = 1) +
theme_minimal(base_size = 16)
@

\subsection{Error cuadrático medio integrado}

El problema con el \(\mathrm{MSE}\left(  \hat{f}_h(x)\right)\) es que depende completamente del punto escogido \(x\).

La solución a esto es integrar el MSE.

\begin{align*}
	\mathrm{MISE}\left(  \hat{f}_h(x)\right)
	  & = \mathrm{E}\left[
		\int_{ -\infty}^{\infty} \left\{
		\hat{f}_h(x) - f(x)
		\right\}^2 dx
		\right]                                                       \\
	  & = \int_{ -\infty}^{\infty} \mathrm{E}\left[
		\left\{
		\hat{f}_h(x) - f(x)
		\right\}^2
		\right] dx                                                    \\
	  & = \int_{ -\infty}^{\infty}\mathrm{MSE}(\hat{f}_h(x)) \, dx
\end{align*}

Además,

\begin{align*}
	\mathrm{MISE} (\hat{f}_h(x))
	  & = \int_{ -\infty}^{\infty} \frac{1}{nh} f(x)dx                                                                                                                                          \\
	  & + \int_{ -\infty}^{\infty}\, \sum_{j}^{} I(x\in B_j) \left\{ \left( j- \frac{1}{2} \right)h -x  \right\}^2 \left [f^\prime \left( \left\{j - \frac{1}{2}\right\}h \right)  \right]^2 dx \\
	  & = \frac{1}{nh} + \sum_{j}^{} \left [f^\prime \left( \left\{j - \frac{1}{2}\right\}h \right)  \right]^2 \int_{ B_j}    \left\{ \left( j- \frac{1}{2} \right)h -x  \right\}^2 dx          \\
	  & =\frac{1}{nh} + \frac{h^2}{12} \sum_{j} \left [f^\prime \left( \left\{j - \frac{1}{2}\right\}h \right)  \right]^2                                                                       \\
	  & \approx \frac{1}{nh} + \frac{h^2}{12} \int \{f^\prime(x)\}^2 dx                                                                                                                         \\
	  & =\frac{1}{nh} + \frac{h^2}{12} \Vert f^\prime\Vert_{2}^2
\end{align*}

\subsection{Ancho de banda óptimo para el histograma}

El MISE tiene el mismo comportamiento que el MSE. Figura \ref{fig:MISE-histograma} presenta el comportamiento de la varianza, sesgo y MISE para nuestro ejemplo.

<<MISE-histograma, fig.cap=' ',echo=FALSE>>=
ggplot(df_MISE) +
geom_line(aes(x=h, y=Valor, color=Medida, linetype=Medida), size=1) +
theme_minimal(base_size = 16)
@

La mala elección del parámetro $h$ causa que el histograma no capture toda la estructura de los datos.

<<echo=FALSE>>=

r <- rnorm(1000)
df <- rbind(
data.frame(`Ancho de banda` = 0.1, x = r),
data.frame(`Ancho de banda` = 0.5, x = r),
data.frame(`Ancho de banda` = 1, x = r),
data.frame(`Ancho de banda` = 1.5, x = r)
)

ggplot(df) +
geom_histogram(aes(x = x, y = stat(density)),
data = subset(df, Ancho.de.banda == 0.1),
binwidth = .01,
fill = "white",
color = "red3"
) +
geom_histogram(aes(x = x, y = stat(density)),
data = subset(df, Ancho.de.banda == 0.5),
binwidth = .05,
fill = "white",
color = "red3"
) +
geom_histogram(aes(x = x, y = stat(density)),
data = subset(df, Ancho.de.banda == 1),
binwidth = 1,
fill = "white",
color = "red3"
) +
geom_histogram(aes(x = x, y = stat(density)),
data = subset(df, Ancho.de.banda == 1.5),
binwidth = 1.5,
fill = "white",
color = "red3"
) +
geom_rug(aes(x), col = "blue4") +
facet_wrap(~ Ancho.de.banda) +
theme_minimal()

@

En este caso se puede simplemente minimizar el MISE de la forma usual,

\begin{equation*}
	\frac{\partial \mathrm{MISE}(f_{h})}{\partial h} = -\frac{1}{nh^2} + \frac{1}{6} h \Vert f^\prime\Vert_{2}^2 = 0
\end{equation*}

implica que

\begin{equation*}
	h_{opt} = \left(\frac{6}{n\Vert f^\prime\Vert_{2}^2}\right) ^{1/3} = O\left( n^{1/3} \right).
\end{equation*}

y que por lo tanto

\begin{equation*}
	\mathrm{MISE}(\hat{f}_{h}) = \frac{1}{n} \left(\frac{n\Vert f^\prime\Vert_{2}^2}{6}\right)  ^{1/3}
\end{equation*}

\begin{nota}{Recuerde de Estadística I}{}

	Si \(X_1, \ldots, X_2 \sim f_{\theta} \) i.i.d, con \(\mathrm{Var}(X) = \sigma^2\), recuerde que el estimador \(\hat{\theta}\)  de \(\theta\) tiene la característica que

	\begin{equation*}
		\mathrm{MSE}(\theta) = \mathrm{Var}(\hat{\theta}) +
		\mathrm{Sesgo}^2(\hat{\theta}) = \frac{\sigma^2}{n}
	\end{equation*}
\end{nota}

Según la nota anterior la tasas de convergencia del histograma es más lenta que la de un estimador parámetrico considerando la misma cantidad de datos.

<<echo=FALSE>>=
nvec <- 5:50

df_rates <-
rbind(
data.frame(
`Tipo Convergencia` = "Paramétrica",
n = nvec,
tasa = nvec^(-1)
),
data.frame(
`Tipo Convergencia` = "Histograma",
n = nvec,
tasa = nvec ^ (-2 / 3)
)
)

ggplot(df_rates) +
geom_line(aes(x = n, y = tasa, color = Tipo.Convergencia)) +
theme_minimal() +
theme(axis.text = element_blank(), axis.line = element_line(size = 1))
@

<<echo=FALSE>>=
# h optimal for the point x0
h_x0 = hvec[MSE == min(MSE)]
# h optimal for any point using the minimal MISE
h_opt_MISE = hvec[MSE == min(MSE)]
# h optimal for any point using the rule-of-thumb
h_opt = (6 / (n * norm_f_prime)) ^ (1 / 3)
# histogram with h_opt
breaks = floor((min(x)) / h_opt):ceiling((max(x)) / h_opt)
breaks = breaks * h_opt
@

Finalmente, podemos encontrar el valor óptimo  de esta datos dado por $h=\Sexpr{h_opt_MISE}$
<<echo=FALSE>>=
df <- data.frame(x)
ggplot(df) +
geom_histogram(
aes(x = x, y = stat(density)),
binwidth = h_opt_MISE,
fill = "white",
color = "red3"
) +
geom_rug(aes(x), col = "blue4") +
geom_density(aes(x = x)) + theme_minimal()
@

<<echo=FALSE>>=
@
\newpage

\section{Estimación No-paramétrica de densidad}

\subsection{Primera construcción}

Sea $X_{1},\ldots,X_{n}$ variables aleatorias i.i.d. con distribución $f$ en $\mathbb{R}$.

La distribución de  $f$ es  $F(x)=\int_{-\infty}^{x}f(t)dt$.

Considere la distribución empírica como
\[
	F_{n}(x)=\frac{1}{n}\sum_{i=1}^{n}I(X_{i}\leq x).
\]

Por la ley de los grandes números tenemos que \(\hat{F}_{n}(x)
\xrightarrow{c.s} F(x)\) para todo  $x$ en $\mathbb{R}$as
$n\rightarrow\infty$. Entonces, $F_{n}(x)$ es consistente

para todo $x$ in $\mathbb{R}$.

\begin{pregunta}{}{}
	?`Podríamos derivar \(\hat{F}_n\) para encontrar el estimar \(\hat{f}_n\)?
\end{pregunta}

La respuesta es si (más o menos).
\newpage
Suponga que $h>0$ tenemos la aproximación
\[
	f(x)\approx\frac{F(x+h)-F(x-h)}{2h}.
\]

Remplazando $F$  por su estimador  $\hat{F}_{n}$, defina
\[
	\hat{f}_{n}^{R}(x)=\frac{F_{n}(x+h)-F_{n}(x-h)}{2h},
\]
donde $\hat{f}_{n}^{R}(x)$ es el estimador de \emph{Rosenblatt }.

Podemos rescribirlo de la forma,
\[
	\hat{f}_{n}^{R}(x)=\frac{1}{2nh}\sum_{i=1}^{n}I(x-h<X_{i}\leq x+h)=\frac{1}{nh}\sum_{i=1}^{n}K_{0}\left(\frac{X_{i}-x}{h}\right)
\]
con  $K_{0}(u)=\frac{1}{2}I(-1<u\leq1)$, lo cuál es equivalente al caso del histograma.

\newpage

\subsection{Otra construcción}

Con el histograma construimos una serie de segmentos fijo \(B_{j}\) y contabamos el número de datos que estaban \textbf{CONTENIDOS} en \(B_{j}\)

\begin{pregunta}{}{}
	¿Qué pasaría si cambiamos la palabra \textbf{CONTENIDOS} por \textbf{ALREDEDOR DE ``x''}?
\end{pregunta}

Suponga que se tienen intervalos de longitud $ 2h $, es decir, intervalos de la forma $ [x-h,x+h) $.

El histograma  se escribe como

\begin{equation*}
	\hat{f_{h}}(x) = \dfrac{1}{2hn} \# \{ X_i \in [x-h,x+h) \}.
\end{equation*}

Ahora tratemos de modificar ligeramente esta expresión notando dos cosas

\begin{enumerate}
	\item \begin{equation*}
		      \frac{1}{2} I \left( \left\vert u \right\vert \leq 1 \right)
	      \end{equation*}
	      con \(u = \frac{x-xi}{h}\)
	\item

	      \begin{equation*}
		      \frac{1}{2}\# \{ X_i \in [x-h,x+h) \}
		      =\sum_{i=1}^{n} K\left( \frac{x-x_{i}}{h} \right)
		      =\sum_{i=1}^{n}  \frac{1}{2} I \left( \left\vert \frac{x-x_{i}}{h}
		      \right\vert \leq 1 \right)
	      \end{equation*}
\end{enumerate}

Finalmente se tiene que

\begin{equation*}
	\hat{f}_{h}\left( x \right) = \frac{1}{nh}\sum_{i=1}^{n} K\left( \frac{x-x_{i}}{h} \right)
\end{equation*}

\begin{center}
	\includegraphics[width=\linewidth]{manual_figure/np-density-interval-crop.pdf}
\end{center}

\newpage
\begin{pregunta}{}{}
	¿Qué pasaría si cambiaríamos la función \(K\) del histograma por una más general?
\end{pregunta}

Esta función debería cumplir las siguientes características

\begin{itemize}
	\item \(K(u)\geq 0\).
	\item \(\int_{-\infty}^{\infty} K(u)du = 1 \).
	\item \(\int_{-\infty}^{\infty} u K(u)du = 0\).
	\item \(\int_{-\infty}^{\infty} u^{2} K(u)du <\infty\).
\end{itemize}

Por ejemplo:

\begin{description}
	\item[Uniforme:] \(\frac{1}{2} I \left( \left\vert u \right\vert \leq 1 \right)\).
	\item[Triangular:] \( (1-|u|) I \left( \left\vert u \right\vert \leq 1 \right)\).
	\item[Epanechnikov:] \(\frac{3}{4} (1-u^{2}) I \left( \left\vert u \right\vert \leq 1 \right)\).
	\item[Gausian:] \(\frac{1}{\sqrt{2\pi}} \exp \left( -\frac{1}{2}u^{2} \right)\).
\end{description}

<<echo=FALSE>>=
# Different Kernel Functions

u = seq(-3, 3, 0.01)

# kernel of uniform distribution
K_Uniform = 0.5 * (abs(u) <= 1)

# kernel of triangle distribution
K_Triangle = (1 - abs(u)) * (abs(u) <= 1)

# epanechnikov kernel
K_Epanechnikov = 0.75 * (1 - u ^ 2) * (abs(u) <= 1)

# kernel of quadratic biweighted distribution
K_Quartic = 0.9375 * (1 - u ^ 2) ^ 2 * (abs(u) <= 1)

# kernel of a gaussian distribution
K_Gaussian = 0.3989 * exp(-0.5 * u ^ 2)

# Plot Kernels

df <- rbind(
data.frame(u, K = K_Uniform, Kernel = "Uniforme"),
data.frame(u, K = K_Epanechnikov, Kernel = "Epanechnikov"),
data.frame(u, K = K_Triangle, Kernel = "Triangular"),
data.frame(u, K = K_Gaussian, Kernel = "Gausiano")
)

ggplot(df, aes(x=u, y=K))+
geom_line() +
facet_wrap(~Kernel,ncol = 2, scales = "free") +
theme_minimal() +
ylab("K(u)") +
theme(axis.line = element_line(size = 1))

@

Entonces se tendría que la expresión general para un estimador por núcleos es

\begin{equation*}
	\hat{f}_{h}\left( x \right) = \frac{1}{nh}\sum_{i=1}^{n} K\left( \frac{x-x_{i}}{h} \right)
\end{equation*}

\newpage

\begin{pregunta}{}{}
	¿Qué pasaría si modificamos el ancho de banda \(h\) para un mismo kernel?
\end{pregunta}

Nuevamente sería el ancho de banda ya que

<<echo=FALSE>>=
r <- rnorm(1000)
df <- rbind(
data.frame(`Ancho de banda` = 0.1, x = r),
data.frame(`Ancho de banda` = 0.5, x = r),
data.frame(`Ancho de banda` = 1, x = r),
data.frame(`Ancho de banda` = 1.5, x = r)
)

ggplot(df) +
geom_density(aes(x = x, y = stat(density)),
data = subset(df, Ancho.de.banda == 0.1),
bw = .01,
fill = "white",
color = "red3"
) +
geom_density(aes(x = x, y = stat(density)),
data = subset(df, Ancho.de.banda == 0.5),
bw = .05,
fill = "white",
color = "red3"
) +
geom_density(aes(x = x, y = stat(density)),
data = subset(df, Ancho.de.banda == 1),
bw = 1,
fill = "white",
color = "red3"
) +
geom_density(aes(x = x, y = stat(density)),
data = subset(df, Ancho.de.banda == 1.5),
bw = 1.5,
fill = "white",
color = "red3"
) +
geom_rug(aes(x), col = "blue4") +
facet_wrap(~ Ancho.de.banda) +
theme_minimal()
@

\newpage

\begin{pregunta}{}{}
	¿Qué pasaría si modificamos el kernel para un mismo ancho de banda \(h\)?
\end{pregunta}

<<echo=FALSE>>=
r <- rnorm(1000)
df <- rbind(
data.frame(Kernel = "gaussian", x = r),
data.frame(Kernel = "epanechnikov", x = r),
data.frame(Kernel = "rectangular", x = r),
data.frame(Kernel = "triangular", x = r)
)

ggplot(df) +
geom_density(aes(x = x, y = stat(density)),
data = subset(df, Kernel == "gaussian"),
bw = 0.3,
kernel = "gaussian",
fill = "white",
color = "red3"
) +
geom_density(aes(x = x, y = stat(density)),
data = subset(df, Kernel == "epanechnikov"),
bw = 0.3,
kernel = "epanechnikov",
fill = "white",
color = "red3"
) +
geom_density(aes(x = x, y = stat(density)),
data = subset(df, Kernel == "rectangular"),
bw = 0.3,
kernel = "rectangular",
fill = "white",
color = "red3"
) +
geom_density(aes(x = x, y = stat(density)),
data = subset(df, Kernel == "triangular"),
bw = 0.3,
kernel = "triangular",
fill = "white",
color = "red3"
) +
geom_rug(aes(x), col = "blue4") +
facet_wrap(~ Kernel) +
theme_minimal()
@

\newpage

Recordemos nuevamente la fórmula

\begin{equation*}
	\hat{f}_{h}\left( x \right) = \frac{1}{nh}\sum_{i=1}^{n} K\left( \frac{x-X_{i}}{h} \right)
\end{equation*}

Cada sumando de esta expresión es una función por si misma. Si la integramos se obtiene que

\begin{equation*}
	\frac{1}{nh}\int K\left( \frac{x-X_{i}}{h} \right) dx
	= \frac{1}{nh} \int K\left( u \right) h du
	= \frac{1}{n} \int K(u) du
	= \frac{1}{n}
\end{equation*}

<<echo=FALSE>>=
set.seed(15)
h <- 0.22
n <- 10

x <- rnorm(n)

xr <- diff(range(x))
ng <- 100
xg <- (xr + 7 * h) * (0:(ng - 1)) / (ng - 1) + min(x) - 3.5 * h

fk <- matrix(0, nrow = ng, ncol = n)
for (j in 1:n) {
		fk[, j] <- dnorm((xg - x[j]) / h) / (n * h)
	}
fh <- rowSums(fk)

ylim <- c(-0.1 * max(fh), max(fh))

p <- ggplot(data.frame(x, xg, fh)) +
geom_line(aes(xg, fh))

for (j in 1:n) {
		p <-
		p + geom_line(data = data.frame(xg, fk = fk[, j]), aes(xg, fk), color = "blue")
	}

p <-
p + geom_point(
data = data.frame(x, xg, fh),
mapping = aes(x = x, y = -0.01),
shape = "x",
color = "red",
size = 6
)
p + theme_minimal() + xlab("x") + ylab(expression(hat(f)[h](x)))

@

\newpage

\subsection{Propiedades Estadísticas}

\begin{pregunta}{}{}
	¿Podríamos imitar lo mismo que hicimos para el histograma?
\end{pregunta}

Si. Las propiedades que vimos anteriormente son universales para estimadores.

Entonces:
\begin{align*}
	\mathrm{MSE}(\hat{f}_{h}(x)) & =\mathrm{Var}(\hat{f}_{h}(x))+\mathrm{Sesgo}^{2} (\hat{f}_{h}(x))            \\
	\mathrm{MISE}(\hat{f}_{h})   & =\int\mathrm{Var}(\hat{f}_{h}(x))dx+\int\mathrm{Sesgo}^{2}(\hat{f}_{h}(x))dx
\end{align*}

donde

\(\mathrm{Var}\left(\hat{f}_{h}(x)\right)=\mathbb{E}\left[\hat{f}_{h}(x)-\mathbb{E}\hat{f}_{h}(x)\right]^{2}\) and \(\mathrm{Sesgo}\left(\hat{f}_{h}(x)\right)=\mathbb{E}\left[\hat{f}_{h}(x)\right]-f(x)\).

\newpage

\subsubsection{Varianza}

\begin{align*}
	\mathrm{Var}(\hat{f}_{h}(x))
	  & =\mathrm{Var}\left(\frac{1}{n}\sum_{i=1}^{n}K\left(\frac{x-X_{i}}{h}\right)\right)          \\
	  & =\frac{1}{n^{2}h^{2}}\sum_{i=1}^{n}\mathrm{Var}\left(K\left(\frac{x-X_{i}}{h}\right)\right) \\
	  & =\frac{1}{nh^{2}}\mathrm{Var}\left(K\left(\frac{x-X}{h}\right)\right)                       \\
	  & =\frac{1}{nh^{2}}\left\{
	\textcolor{red}{\mathbb{E}\left[K^{2}\left(\frac{x-X}{h}\right)\right]}
	-\left\{
	\textcolor{blue}{\mathbb{E}\left[K\left(\frac{x-X}{h}\right)\right]}
	\right\}^{2}
	\right\}.
\end{align*}
Usando que:
\begin{align*}
	\textcolor{red}{\mathbb{E}\left[K^{2}\left(\frac{x-X}{h}\right)\right]}
	  & =\int K^{2}\left(\frac{x-s}{h}\right)f(s)ds            \\
	  & =h\int K^{2}\left(u\right)f(uh+x)du                    \\
	  & =h\int K^{2}\left(u\right)\left\{ f(x)+o(1)\right\} du \\
	  & =h\left\{ \Vert K\Vert_{2}^{2}f(x)+o(1)\right\} .
\end{align*}

\begin{align*}
	\textcolor{blue}{\mathbb{E}\left[K\left(\frac{x-X}{h}\right)\right]}
	  & =\int K\left(\frac{x-s}{h}\right)f(s)ds            \\
	  & =	h\int K\left(u\right)f(uh+x)du                    \\
	  & =h\int K\left(u\right)\left\{ f(x)+o(1)\right\} du \\
	  & =h\left\{f(x)+o(1)\right\} .
\end{align*}

Por lo tanto se obtiene que

\begin{equation*}
	\mathrm{Var}\left(\hat{f}_{h}(x)\right) = \frac{1}{nh} \Vert K\Vert_{2}^{2}f(x) + o\left(\frac{1}{nh}\right), \text{ si } nh\to \infty.
\end{equation*}

\newpage

\subsubsection{Sesgo}

Para el sesgo tenemos

\begin{align*}
	\mathrm{Sesgo}\left(\hat{f}_{h}(x)\right)
	  & = \mathbb{E}\left[\hat{f}_{h}(x)\right]-f(x)                                                  \\
	  & = \frac{1}{nh} \sum_{i=1}^{n} \mathrm{E}\left[K\left( \frac{x-X_{i}}{h} \right)\right] - f(x) \\
	  & = \frac{1}{h}\mathrm{E}\left[K\left( \frac{x-X_{1}}{h} \right)\right] - f(x)                  \\
	  & = \int \frac{1}{h} K\left( \frac{x-u}{h}\right)f(u)du -f(x)                                   \\
\end{align*}

\begin{tarea}{}{}
	Usando el cambio de variable \(s=\frac{u-x}{h}\) y las propiedades del kernel pruebe que

	\begin{equation*}
		\mathrm{Sesgo}\left(\hat{f}_{h}(x)\right) = \frac{h^{2}}{2} f^{\prime\prime} \mu_{2}(K) + o(h^{2}), \text{ si } h\to 0
	\end{equation*}
	donde \(\mu_{2}=\int s^{2}K(s)ds\).

	\emph{\textbf{Nota:} En algunas pruebas más formales, se necesita
	además que  $f^{\prime\prime}$ sea absolutamente continua y que
	$\int(f^{\prime\prime\prime}(x))dx<\infty$.}
\end{tarea}

\begin{solucion}{}{Pba_tarea_Sesgo}
	\begin{align*}
\mathrm{Sesgo}(\hat{f_{h}}(x)) & = \int \frac{1}{h} K\left( \frac{x-u}{h} \right) f(u)du - f(x)		\\
& = \frac{1}{h} \int hK(s)f(sh+x) ds - f(x) \\
 & = \int K(s)\Biggl[ f(x) + f^{\prime}(x)(sh+x-x)  \\
 &  \qquad  + \frac{f^{\prime\prime}(x)}{2}(sh+x-x)^2 + o(h^{2}) \Biggr] - f(x) \\
 & = \int K(s)f(x)ds + \int hf^{\prime}(x)sK(s) ds  \\
 & \qquad  + \int \frac{h^2}{2} f^{\prime\prime}(x)s^2K(s) ds + o(h^2) - f(x) \\
 & = f(x) + 0 + \frac{h^2}{2}f^{\prime\prime}(x)\mu_{2}(K) + o(h^2) - f(x)	\\
 & = \frac{h^2}{2}f^{\prime\prime}(x)\mu_{2}(K) + o(h^2) \\
	\end{align*}
\end{solucion}

<<echo=FALSE>>=
ggplot(faithful, aes(x = eruptions)) +
stat_density(
mapping = aes(color = "black"),
geom = "line",
bw = 0.3,
size = 2
) +
stat_density(
mapping = aes(color = "red"),
geom = "line",
bw = 0.5,
size = 1,
linetype = "dashed",
) +
scale_color_identity(
name = "Ancho\nde banda",
breaks = c("black", "red"),
labels = c("h = 0.3", "h = 0.5"),
guide = "legend"
) +
xlab("x") +
theme_minimal(base_size = 16)
@

\begin{nota}{}{}
	Note como los cambios en el ancho de banda modifican la suavidad (sesgo) y el aplanamiento de la curva (varianza).
\end{nota}

\newpage

\subsubsection{Error cuadrático medio y Error cuadrático medio integrado}

El error cuadrático medio se escribe
\begin{align*}
	\mathrm{MSE}(\hat{f}_{h}(x))
	  & = \mathrm{Sesgo}\left(\hat{f}_{h}(x)\right)^{2} + \mathrm{Var}\left(\hat{f}_{h}(x)\right)                                                 \\
	  & = \frac{h^{4}}{4}\left(\mu_{2}(K)f^{\prime\prime}(x)\right)^{2}+\frac{1}{nh}\Vert K\Vert_{2}^{2}f(x)+o(h^{4})+o\left(\frac{1}{nh}\right).
\end{align*}

Y el error cuadrático medio integrado se escribe como,
\begin{align*}
	\mathrm{MISE}\left(\hat{f}_{h}\right) & = \int \mathrm{MSE}\left(\hat{f}_{h}(x)\right)dx                                                                                                        \\
	                                      & = \int \mathrm{Sesgo}\left(\hat{f}_{h}(x)\right)^{2} + \mathrm{Var}\left(\hat{f}_{h}(x)\right)dx                                                        \\
	                                      & = \frac{h^{4}}{4}\mu_{2}^{2}(K)\left\Vert f^{\prime\prime}(x)\right\Vert_{2}^{2} +\frac{1}{nh}\Vert K\Vert_{2}^{2}+o(h^{4})+o\left(\frac{1}{nh}\right).
\end{align*}
\newpage
\subsubsection{Ancho de banda óptimo}

Minimizando el \(\mathrm{MISE}\) con respecto a \(h\) obtenemos
\begin{equation*}
	h_{opt}=\left(\frac{\Vert K\Vert_{2}^{2}}{\Vert f^{\prime\prime}\Vert_{2}^{2}\left(\mu_{2}(K)\right)^{2}n}\right)^{1/5}=O\left( n^{-1/5} \right).
\end{equation*}

\begin{nota}{}{}
	De forma práctica, $h_{opt}$ no es un estimador útil de $h$ porque depende de $\Vert f^{\prime\prime}\Vert_{2}^{2}$  que es desconocido.

	Más adelante veremos otra forma de encontrar este estimador.
\end{nota}

Evaluando $h_{opt}$ en el \(\mathrm{MISE}\) tenemos que

\begin{equation*}
	\mathrm{MISE}(\hat{f}_{h})=\frac{5}{4}\left(\Vert K\Vert_{2}^{2}\right)^{4/5}\left(\Vert f^{\prime\prime}\Vert_{2}^{2}\mu_{2}(K)\right)^{2/5}n^{-4/5} = O\left( n^{-4/5} \right).
\end{equation*}

<<echo=FALSE>>=
nvec <- 5:50

df_rates <-
rbind(
data.frame(
`Tipo Convergencia` = "Paramétrica",
n = nvec,
tasa = nvec^(-1)
),
data.frame(
`Tipo Convergencia` = "Histograma",
n = nvec,
tasa = nvec ^ (-2 / 3)
),
data.frame(
`Tipo Convergencia` = "Núcleos",
n = nvec,
tasa = nvec ^ (-4 / 5)
)
)

ggplot(df_rates) +
geom_line(aes(x = n, y = tasa, color = Tipo.Convergencia)) +
theme_minimal() +
theme(axis.text = element_blank(), axis.line = element_line(size = 1))
@

\newpage

\begin{nota}{Detalle técnico}{}
	Formalmente, es posible probar que si $f$ es $\beta$ veces continuamente diferenciable y  $\int\left(f^{(\beta)}\right)^{2}<\infty$, entonces se tiene que
	\[
		{\displaystyle h_{opt}=O\left(n^{-\frac{1}{2\beta+1}}\right).}
	\]
	Por lo tanto se podría aproximar a una tasa paramétrica de convergencia si
	\(\beta\) es grande.
\end{nota}

\newpage

\subsection{Escogiendo el ancho de banda}

\begin{nota}{}{}
	La principal característica del ancho de banda
	\begin{equation*}
		h_{opt}=\left(\frac{\Vert K\Vert_{2}^{2}}{\Vert f^{\prime\prime}\Vert_{2}^{2}\left(\mu_{2}(K)\right)^{2}n}\right)^{1/5}=O\left( n^{-1/5} \right).
	\end{equation*}

	ES QUE ¡NO FUNCIONA!

\end{nota}

Veremos dos métodos para determinar un \(h\) que funcione:

\begin{itemize}
	\item Referencia normal.
	\item Validación cruzada.
\end{itemize}

\newpage

\subsubsection{Referencia normal}

\begin{cuidado}{}{}
	Este método es más efectivo si se conoce que la verdadera distribución es bastante suave, unimodal y simétrica.

	Más adelante veremos otro método para densidades más generales.
\end{cuidado}

Asuma que \(f\) es normal distribuida y se utiliza un kernel \(K\) gausiano. Entonces se tiene que

\begin{align*}
	\hat{h}_{rn} & =\left(\frac{\Vert K\Vert_{2}^{2}}{\Vert f^{\prime\prime}\Vert_{2}^{2}\left(\mu_{2}(K)\right)^{2}n}\right)^{1/5}=O\left( n^{-1/5} \right) \\
	             & =1.06 \hat{\sigma} n^{-1/5}.
\end{align*}

donde

\begin{equation*}
	\hat{\sigma} = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} \left( x_{i}-\bar{x}^{2} \right)}
\end{equation*}

\begin{tarea}{}{}
	Pruebe que la ecuación anterior es verdadera. Es decir, calcule \(\Vert K\Vert_{2}^{2}\), \(\Vert f^{\prime\prime}\Vert_{2}^{2}\) y \(\mu_{2}(K)\)
\end{tarea}

\begin{nota}{}{}
	Un problema con \(\hat{h}_{rn}=1.06 \hat{\sigma} n^{-1/5}\) es su sensibilidad a los valores extremos.
\end{nota}

\begin{ejemplo}{}{}
	La varianza empírica de  1, 2, 3, 4, 5, es  2.5.

	La varianza empírica de 1, 2, 3, 4, 5, 99, es 1538.
\end{ejemplo}

El rango intercuantil IQR se define como
\begin{equation*}
	\mathrm{IQR}^{X} = Q^{X}_{3} - Q^{X}_{1}
\end{equation*}
donde \(Q^{X}_{1}\) y \(Q^{X}_{3}\) son el primer y tercer  de un conjunto de datos \(X_{1},\ldots, X_n\).

Con el supuesto que \(X\sim \mathcal{N}(\mu,\sigma^{2})\) entonces \(\displaystyle Z = \frac{X-\mu}{\sigma} \sim \mathcal{N}(0,1)\).

Entonces,
\begin{align*}
	\mathrm{IQR}
	  & = Q^{X}_{3} - Q^{X}_{1}                                                     \\
	  & = \left( \mu+\sigma Q^{Z}_{3} \right) - \left( \mu+\sigma Q^{Z}_{1} \right) \\
	  & = \sigma \left(Q^{Z}_{3} - Q^{Z}_{1} \right)                                \\
	  & \approx \sigma \left( 0.67 - (0.67) \right)                                 \\
	  & =1.34 \sigma.
\end{align*}

Por lo tanto \(\displaystyle \hat{\sigma} = \frac{\widehat{\mathrm{IQR}}^{X}}{1.34}\)

Podemos sustituir la varianza empírica de la fórmula inicial y tenemos
\begin{equation*}
	\hat{h}_{rn} = 1.06 \frac{\widehat{\mathrm{IQR}}^{X}}{1.34} n^{-\frac{1}{5}} \approx 0.79\  \widehat{\mathrm{IQR}}^{X}\ n^{-\frac{1}{5}}
\end{equation*}

Combinando ambos estimadores, podemos obtener,

\begin{equation*}
	\hat{h}_{rn} = 1.06 \min \left\{\frac{\widehat{\mathrm{IQR}}^{X}}{1.34}, \hat{\sigma }\right\} n^{-\frac{1}{5}}
\end{equation*}

\newpage

\subsubsection{Validación Cruzada}

Defina el \emph{error cuadrático integrado} como
\begin{align*}
	\mathrm{ISE}(\hat{f}_{h}) & =\int\left(\hat{f}_{h}(x)-f(x)\right)^{2}dx\nonumber                   \\
	                          & =\int \hat{f}_{h}^{2}(x)dx-2\int \hat{f}_{h}(x)f(x)dx+\int f^{2}(x)dx.
\end{align*}

\begin{nota}{}{}
	El MISE es el valor esperado del ISE.
\end{nota}

Nuestro objetivo es minimizar el ISE con respecto a \(h\).

Primero note que \(\int f^{2}(x)dx\) NO DEPENDE de \(h\). Podemos minimizar la expresión
\begin{equation*}
	\mathrm{ISE}(\hat{f}_{h})-\int f^{2}(x)dx=
	\textcolor{red}{\int\hat{f}_{h}^{2}(x)dx}
	-2
	\textcolor{blue}{\int\hat{f}_{h}(x)f(x)dx}
\end{equation*}

Vamos a resolver esto en dos pasos partes

\newpage

\paragraph{ Integral \(\textcolor{blue}{\int\hat{f}_{h}(x)f(x)dx}\)}  \ \newline

El término \(\textcolor{blue}{\int\hat{f}_{h}(x)f(x)dx}\) es el valor esperado de
\(\mathrm{E}\left[\hat{f}(X)\right]\). Su estimador es
\begin{equation*}
	\widehat{\mathrm{E}\left[\hat{f}(X)\right]}
	= \frac{1}{n}\sum_{i=1}^{n}\hat{f}_{h}(X_{i})
	=\frac{1}{n^{2}h}\sum_{i=1}^{n}\sum_{j=1}^{n}
	K\left(\frac{X_{j}-X_{i}}{h}\right).
\end{equation*}

\begin{cuidado}{}{}
	El problema con esta expresión es que las observaciones que se usan para estimar la esperanza son las misma que se usan para estimar \(\hat{f}_{h}(x)\) (Se utilizan doble).
\end{cuidado}

La solución es remover la  $i^{\text{ésima}}$ observación de $\hat{f}_{h}$ para cada \(i\).

Redefiniendo el estimador anterior tenemos $\int \hat{f}_{h}(x)f(x)dx$ como
\[
	\frac{1}{n}\sum_{i=1}^{n}\hat{f}_{h,-i}(X_{i}),
\]
donde
\[
	\hat{f}_{h,-i}(x)=\frac{1}{(n-1)h}\sum_{\substack{j=1\\ j\neq i}}^{n}K\left( \frac{x-X_{i}}{h} \right) .
\]
\newpage

\paragraph{ Integral \(\textcolor{red}{\int\hat{f}_{h}^{2}(x)dx}\)}  \ \newline

Siguiendo con el término \(\textcolor{red}{\int\hat{f}_{h}^{2}(x)dx}\) note que este se puede reescribir como

\begin{align*}
	\textcolor{red}{\int\hat{f}_{h}^{2}(x)dx}
	  & =\int\left(\frac{1}{nh}\sum_{i=1}^{n}K\left( \frac{x-X_{i}}{h} \right)\right)^{2}dx                                    \\
	  & =\frac{1}{n^{2}h^{2}}\sum_{i=1}^{n}\sum_{i=1}^{n}\int K\left(\frac{x-X_{i}}{h}\right)K\left(\frac{x-X_{j}}{h}\right)dx \\
	  & =\frac{1}{n^{2}h}\sum_{i=1}^{n}\sum_{i=1}^{n}\int K\left(u\right)K\left(\frac{X_{i}-X_{j}}{h}-u\right)du               \\
	  & =\frac{1}{n^{2}h}\sum_{i=1}^{n}\sum_{i=1}^{n}K*K\left(\frac{X_{i}-X_{j}}{h}\right).
\end{align*}

donde $K*K$ es la convolución de  $K$  consigo misma.

\newpage
Finalmente tenemos la  función,

\[
	\mathrm{CV}(h)=\frac{1}{n^{2}h}\sum_{i=1}^{n}\sum_{j=1}^{n}K*K\left(\frac{X_{i}-X_{j}}{h}\right)-\frac{2}{(n-1)h}\sum_{i=1}^{n}\mathop{\sum_{j=1}^{n}}_{j\neq i}K\left( \frac{X_{i}-X_{j}}{h} \right).
\]

\begin{nota}{}{}
	Note que \(\mathrm{CV}(h)\) no depende de \(f\) o sus derivadas.
\end{nota}

\begin{nota}{}{}
	Para efectos prácticos es mejor utilizar el criterio

	\[
		CV(h)=\int\hat{f}_{h}^{2}(x)dx-\frac{2}{(n-1)h}\sum_{i=1}^{n}\mathop{\sum_{j=1}^{n}}_{j\neq i}K\left( \frac{X_{i}-X_{j}}{h} \right)
	\]
	y luego calcular numéricamente la integral.
\end{nota}

\newpage

\subsection{Intervalos de confianza para estimadores de densidad no paramétricos}

Usando los resultados anteriores y asumiendo que \(h=cn^{-\frac{1}{5}}\) entonces

\begin{equation*}
	n^{-\frac{2}{5}} \left\{ \hat{f}_{h}(x) -f(x)\right\}
	\xrightarrow{\mathcal{L}} \mathcal{N}\left(\underbrace{\frac{c^{2}}{2} f^{\prime\prime}
		\mu_{2}(K)}_{b_{x}}, \underbrace{\frac{1}{c}f(x) \left\Vert K \right\Vert_{2}^{2}}_{v_{x}}\right).
\end{equation*}

Si \(z_{1-\frac{\alpha}{2}}\) es el cuantil \(1-\frac{\alpha}{2}\) de una distribución normal estándar, entonces

\begin{align*}
	1-\alpha
	  & \approx \mathbb{P}\left(b_{x}-z_{1-\frac{\alpha}{2}} v_{x} \leq n^{2 / 5}\left\{\widehat{f}_{h}(x)-f(x)\right\} \leq b_{x}+z_{1-\frac{\alpha}{2}} v_{x}\right) \\
	  & =\mathbb{P}\left(\widehat{f}_{h}(x)-n^{-2 / 5}\left\{b_{x}+z_{1-\frac{\alpha}{2}} v_{x}\right\}\right.                                                         \\
	  & \qquad\qquad \left. \leq f(x)\leq \hat{f}_{h}(x)-n^{-2 / 5}\left\{b_{x}-z_{1-\frac{\alpha}{2}} v_{x}\right\}\right)
\end{align*}

Esta expresión nos dice que con una probabilidad de  \(1-\alpha\) se tiene que

\begin{equation*}
	\begin{aligned}
		  & \left[\hat{f}_{h}(x)-\frac{h^{2}}{2} f^{\prime \prime}(x) \mu_{2}(K)-z_{1-\frac{\alpha}{2}} \sqrt{\frac{f(x)\|K\|_{2}^{2}}{n h}}\right. \\
		  & \left.\widehat{f}_{h}(x)-\frac{h^{2}}{2} f^{\prime \prime}(x) \mu_{2}(K)+z_{1-\frac{a}{2}} \sqrt{\frac{f(x)\|K\|_{2}^{2}}{n h}}\right]
	\end{aligned}
\end{equation*}

Al igual que en los casos anteriores, este invtervalo no es útil ya que depende de \(f(x)\) y \(f^{\prime\prime} (x)\).

Si \(h\) es pequeño relativamente a \(n^{-\frac{1}{5}}\) entonces el segundo término \(\frac{h^{2}}{2} f^{\prime \prime}(x) \mu_{2}(K)\) podría ser ignorado.

Podemos reemplazar \(f(x)\) por su estimador \(\hat{f}_{h}(x)\).  Entonces tendríamos una intervalo aplicable a nuestro caso

\begin{equation*}
	\left[\hat{f_{h}}(x)-z_{1-\frac{\alpha}{2}} \sqrt{\frac{\hat{f_{h}}(x)\|K\|_{2}^{2}}{n h}}, \hat{f}_{h}(x)+z_{1-\frac{\alpha}{2}} \sqrt{\frac{\hat{f}_{h}(x)\|\mathrm{K}\|_{2}^{2}}{n h}}\right]
\end{equation*}

\begin{nota}{}{}
	Este intervalo de confianza solo funciona en cada punto particular de \(f(x)\).

	Existe una versión más general para determinar la banda de confianza de toda la función. Por favor revisar la página 62 de \textcite{Hardle2004}.
\end{nota}
\newpage
\subsection{Laboratorio}

Comenzaremos con una librería bastante básica llamada \texttt{KernSmooth}.

\subsubsection{ Efecto de distintos Kernels en la estimación }

<<>>=
x <- read.csv("data/stockres.txt")
x <- x$STOCKRETURN
@



 <<>>=
 summary(x)
 @


 <<>>=
library(KernSmooth)

fhat_normal <- bkde(x, kernel = "normal", bandwidth = 0.05)
plot(fhat_normal, type = "l")

fhat_unif <- bkde(x, kernel = "box", bandwidth = 0.05)
plot(fhat_unif, type = "l")

fhat_epanech <- bkde(x, kernel = "epanech", bandwidth = 0.05)
plot(fhat_epanech, type = "l")

fhat_biweight <- bkde(x, kernel = "biweight", bandwidth = 0.05)
plot(fhat_biweight, type = "l")

fhat_triweight <- bkde(x, kernel = "triweight", bandwidth = 0.05)
plot(fhat_triweight, type = "l")
 @
 \subsubsection{Efecto del ancho de banda en la estimación }

 \paragraph{Kernel uniforme}
 <<>>=
fhat <- bkde(x, kernel = "box", bandwidth = 0.001)
plot(fhat, type = "l")

fhat <- bkde(x, kernel = "box", bandwidth = 0.5)
plot(fhat, type = "l")
 @

 \paragraph{Kernel Epanechnikov}

 <<>>=
fhat <- bkde(x, kernel = "epa", bandwidth = 0.001)
plot(fhat, type = "l")

fhat <- bkde(x, kernel = "epa", bandwidth = 0.5)
plot(fhat, type = "l")
 @

 <<fig.keep='none', eval=FALSE>>=
suppressMessages(library(tidyverse))
library(gganimate)

fani <- tibble()

for (b in seq(0.001, 0.02, length.out = 40)) {
  f <- bkde(x, kernel = "epa", bandwidth = b, gridsize = length(x))
  fani <- fani %>% bind_rows(tibble(xreal = sort(x), x = f$x, y = f$y, bw = b))
}

ggplot(data = fani) +
  geom_line(aes(x, y), color = "blue") +
  labs(title = paste0("Ancho de banda = {closest_state}")) +
  transition_states(bw) +
  view_follow() +
  theme_minimal(base_size = 20)

anim_save("manual_figure/bandwidth-animation.gif")
 @

 \includemedia[
 label=bandwidth,
 width=0.6\linewidth,height=0.45\linewidth,
 addresource=manual_figure/bandwidth-animation.mp4,
 transparent,
 %transparent player background
 activate=pageopen,
 %show VPlayer's right-click menu
 flashvars={
 source=manual_figure/bandwidth-animation.mp4
 &loop=true
 % loop video
 }
 ]{}{VPlayer.swf}

 \begin{pregunta}{}{}

 \begin{enumerate}
 \item Construya una variable llamada `u` que sea una secuencia de -0.15 a 0.15 con un paso de 0.01
 \item Asigne `x` a los datos `stockrel` y calcule su media y varianza.
 \item Usando la función `dnorm` construya los valores de la distribución de los datos usando la media y varianza calculada anteriormente. Asigne a esta variable `f\_param`.
 \item Defina un ancho de banda `h` en 0.02
 \item Construya un histograma para estos datos con ancho de banda `h`. Llame a esta variable `f\_hist`
 \item Usando el paquete `KernSmooth` y la función `bkde`, construya una función que calcule el estimador no paramétrico con un núcleo Epanechivok para un ancho de banda  $h$.  Llame a esta variable `f\_epa`.
 \item Dibuje en el mismo gráfico la estimación paramétrica y no paramétrica.
 \end{enumerate}
 \end{pregunta}

 \begin{solucion}{}{}
 <<>>=
x <- read.csv("data/stockres.txt")
x <- unlist(x)
# Eliminar nombres de las columnas
names(x) <- NULL

u <- seq(-0.15, 0.15, by = 0.01)

mu <- mean(x)
sigma <- sd(x)

f_param <- dnorm(u, mean = mu, sd = sigma)

h <- 0.02

n_bins <- floor(diff(range(x)) / h)

f_hist <- hist(x, breaks = n_bins)

f_epa <- as.data.frame(bkde(x, kernel = "epa", bandwidth = h))

x_df <- data.frame(x)

library(ggplot2)

ggplot(x_df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = 0.02,
    col = "black",
    fill = "white"
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sigma),
    color = "red"
  ) +
  geom_line(data = f_epa, aes(x, y), color = "blue") +
  theme_minimal(base_size = 20)
 @

 \end{solucion}

 \subsubsection{Ancho de banda óptimo}

 Usemos la regla de la normal o también conocida como Silverman.
 \textbf{Primero recuerde que en este caso se asume que $f(x)$ sigue una distribución normal}. En este caso, lo que se obtiene es que

 \begin{align*}
 \Vert f^{\prime \prime} \Vert_2^2 & = \sigma ^{-5} \int \{\phi^{\prime \prime}\}^2 dx              \\
 & = \sigma ^{-5} \frac{3}{8\sqrt{\pi}} \approx 0.212 \sigma^{-5}
 \end{align*}

 donde $\phi$ es la densidad de una normal estándar.

 El estimador para $\sigma$ es

 \[
 s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2  }.
 \]

 Y usando el cálculo realizado anteriormente, se obtiene que

 \[
 h_{normal} = \left( \frac{4 s^5}{3n} \right)^{1/5} \approx 1.06 s n^{-1/5}.
 \]

 Un estimador más robusto es

 \[
 h_{normal} =  1.06 \min \left\{ s , \frac{IQR}{1.34} \right\} n^{-1/5}.
 \]

 ¿Por qué es $IQR / 1.34$?

 <<>>=
 s <- sd(x)
 n <- length(x)
 @

 <<>>=
 h_normal <- 1.06 * s * n^(-1 / 5)

h <- h_normal

n_bins <- floor(diff(range(x)) / h)
f_hist <- hist(x, breaks = n_bins, plot = FALSE)
f_epa <- as.data.frame(bkde(x, kernel = "epa", bandwidth = h))

ggplot(x_df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = h,
    col = "black",
    fill = "white"
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sigma),
    color = "red"
  ) +
  geom_line(data = f_epa, aes(x, y), color = "blue") +
  theme_minimal(base_size = 20)
 @

 <<>>=
h_iqr <- 1.06 * min(s, IQR(x) / 1.34) * n^(-1 / 5)

h <- h_iqr

n_bins <- floor(diff(range(x)) / h)
f_hist <- hist(x, breaks = n_bins, plot = FALSE)
f_epa <- as.data.frame(bkde(x, kernel = "epa", bandwidth = h))

ggplot(x_df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = h,
    col = "black",
    fill = "white"
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sigma),
    color = "red"
  ) +
  geom_line(data = f_epa, aes(x, y), color = "blue") +
  theme_minimal(base_size = 20)
 @

 Una librería más especializada es \texttt{np} (non-parametric).

 <<>>=
 library(np)

x.eval <- seq(-0.2, 0.2, length.out = 200)

h_normal_np <- npudensbw(dat = x, bwmethod = "normal-reference")

dens.ksum <- npksum(
  txdat = x,
  exdat = x.eval,
  bws = h_normal_np$bw
)$ksum / (n * h_normal_np$bw[1])

dens.ksum.df <- data.frame(x = x.eval, y = dens.ksum)

ggplot(x_df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = h_normal_np$bw,
    col = "black",
    fill = "white"
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sigma),
    color = "red"
  ) +
  geom_line(data = dens.ksum.df, aes(x, y), color = "blue") +
  theme_minimal(base_size = 20)
 @

 \subsubsection{Validación cruzada}

 La forma que vimos en clase es la de validación cruzada por mínimos
 cuadrados``least-square cross validation'' la cual se puede ejecutar
 con este comando.

 <<>>=
h_cv_np_ls <- npudensbw(
  dat = x,
  bwmethod = "cv.ls",
  ckertype = "epa",
  ckerorder = 2
)

dens.np <- npudens(h_cv_np_ls)

plot(dens.np, type = "b")

 @

 <<>>=
dens.np.df <- data.frame(
  x = dens.np$eval[, 1],
  y = dens.np$dens
)

ggplot(x_df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = h_cv_np_ls$bw,
    col = "black",
    fill = "white"
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sigma),
    color = "red"
  ) +
  geom_line(data = dens.np.df, aes(x, y), color = "blue") +
  theme_minimal(base_size = 20)
 @

 \subsubsection{Temas adicionales}

 \paragraph{Reducción del sesgo}
 Como lo mencionamos en el texto, una forma de mejorar el sesgo en la estimación es suponer que la función de densidad es más veces diferenciable.

 Esto se logra asumiendo que el Kernel es más veces diferenciable.

 <<>>=
h_cv_np_ls <- npudensbw(
  dat = x,
  bwmethod = "cv.ls",
  ckertype = "epa",
  ckerorder = 4
)

dens.np <- npudens(h_cv_np_ls)

plot(dens.np, type = "b", lwd = 2)
 @

 <<>>=
dens.np.df <- data.frame(x = dens.np$eval[, 1], y = dens.np$dens)

ggplot(x_df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = h_cv_np_ls$bw,
    col = "black",
    fill = "white"
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sigma),
    color = "red"
  ) +
  geom_line(data = dens.np.df, aes(x, y), color = "blue") +
  theme_minimal(base_size = 20)
 @

 \paragraph{Otra forma de estimar el ancho de banda} Otra forma de estimar ancho de bandas óptimos es usando máxima verosimilitud. Les dejo de tarea revisar la sección 1.1 del artículo de~\textcite{Hall1987} para entender su estructura.

 <<>>=
h_cv_np_ml <- npudensbw(
  dat = x,
  bwmethod = "cv.ml",
  ckertype = "epanechnikov"
)

dens.np <- npudens(h_cv_np_ml)

plot(dens.np, type = "b")
 @

 <<>>=
 dens.np.df <- data.frame(x = dens.np$eval[, 1], y = dens.np$dens)

ggplot(x_df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = h_cv_np_ml$bw,
    col = "black",
    fill = "white"
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sigma),
    color = "red"
  ) +
  geom_line(data = dens.np.df, aes(x, y), color = "blue") +
  theme_minimal(base_size = 20)
 @

 <<>>=
h_cv_np_ml <- npudensbw(
  dat = x,
  bwmethod = "cv.ml",
  ckertype = "epanechnikov",
  ckerorder = 4
)

dens.np <- npudens(h_cv_np_ml)

plot(dens.np, type = "b")
 @

 <<>>=
dens.np.df <- data.frame(x = dens.np$eval[, 1], y = dens.np$dens)

ggplot(x_df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = h_cv_np_ml$bw,
    col = "black",
    fill = "white"
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sigma),
    color = "red"
  ) +
  geom_line(data = dens.np.df, aes(x, y), color = "blue") +
  theme_minimal(base_size = 20)
 @

 <<eval=FALSE>>=
 fani <- tibble()

for (b in seq(0.001, 0.05, length.out = 40)) {
  f <-
    npudens(
      tdat = x,
      ckertype = "epanechnikov",
      bandwidth.compute = FALSE,
      bws = b
    )
  fani <-
    fani %>% bind_rows(tibble(
      xreal = sort(x),
      x = f$eval$x,
      y = f$dens,
      bw = b
    ))
}

ggplot(data = fani) +
  geom_line(aes(x, y), color = "blue") +
  labs(title = paste0("Ancho de banda = {closest_state}")) +
  transition_states(bw) +
  view_follow() +
  theme_minimal(base_size = 20)

anim_save("manual_figure/bandwidth-animation-np.gif")
 @
 \includemedia[
 label=bandwidth,
 width=0.6\linewidth,height=0.45\linewidth,
 addresource=manual_figure/bandwidth-animation-np.mp4,
 transparent,
 %transparent player background
 activate=pageopen,
 %show VPlayer's right-click menu
 flashvars={
 source=manual_figure/bandwidth-animation-np.mp4
 &loop=true
 % loop video
 }
 ]{}{VPlayer.swf}

 \begin{tarea}{}{}
 Implementar el intervalo confianza visto en clase para estimadores de densidades por núcleos y visualizarlo de en ggplot. 

 \textbf{Si se atreven: ¿Se podría hacer una versión animada de ese gráfico para visualizar el significado real de este el intervalo de confianza?}
 \end{tarea}


